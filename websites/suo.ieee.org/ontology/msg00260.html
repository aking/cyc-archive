<!-- MHonArc v2.4.8 -->
<!--X-Subject: SUO: Re: Arete, Episteme, Techne -->
<!--X-From-R13: Xba Ojoerl <wnjoerlNbnxynaq.rqh> -->
<!--X-Date: Wed, 3 Jan 2001 00:12:33 &#45;0500 (EST) -->
<!--X-Message-Id: 3A52B1D1.E74B1841@oakland.edu -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: 3A51F290.15AA8B5E@oakland.edu -->
<!--X-Reference: 3A51F577.1D681085@oakland.edu -->
<!--X-Reference: 3A521108.A75F7743@oakland.edu -->
<!--X-Head-End-->

<!-- /groups/802/3/efm/public/email/msg00260.html -->
<!-- /groups/???? ?SUO?                              -->

<HTML>

<HEAD>
<TITLE>SUO: Re: Arete, Episteme, Techne</TITLE>
<LINK REV="made" HREF="mailto:jawbrey@oakland.edu">
</HEAD>

<BODY BGCOLOR="#FFFFFF">

<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->

<CENTER>

<TABLE CELLPADDING=3 CELLSPACING=0 BORDER=1 WIDTH="100%">
<TBODY>
<TR ALIGN="CENTER">
<TH COLSPAN=3><STRONG>Thread Links</STRONG></TH>
<TH COLSPAN=3><STRONG>Date Links</STRONG></TH>
</TR>
<TR ALIGN="CENTER">
<TD><A HREF="msg00258.html">Thread Prev</A>
</TD>
<TD><A HREF="msg00268.html">Thread Next</A>
</TD>
<TD><A HREF="thrd83.html#00260">Thread Index</A></Td>
<TD><A HREF="msg00261.html">Date Prev</A></TD>
<TD><A HREF="msg00259.html">Date Next</A>
</TD>
<TD><A HREF="mail85.html#00260">Date Index</A></TD>
</TR>
</TBODY>
</TABLE>
</CENTER>


<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h1>SUO: Re: Arete, Episteme, Techne</h1>
<hr>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<UL>
<LI><em>To</em>: Arisbe &lt;<A HREF="mailto:arisbe@stderr.org">arisbe@stderr.org</A>&gt;,       Stand Up Ontology &lt;<A HREF="mailto:standard-upper-ontology@ieee.org">standard-upper-ontology@ieee.org</A>&gt;</LI>
<LI><em>Subject</em>: SUO: Re: Arete, Episteme, Techne</LI>
<LI><em>From</em>: Jon Awbrey &lt;<A HREF="mailto:jawbrey@oakland.edu">jawbrey@oakland.edu</A>&gt;</LI>
<LI><em>Date</em>: Wed, 03 Jan 2001 00:00:01 -0500</LI>
<LI><em>References</em>: &lt;<A HREF="msg00256.html">3A51F290.15AA8B5E@oakland.edu</A>&gt; &lt;<A HREF="msg00257.html">3A51F577.1D681085@oakland.edu</A>&gt; &lt;<A HREF="msg00258.html">3A521108.A75F7743@oakland.edu</A>&gt;</LI>
<LI><em>Reply-To</em>: Jon Awbrey &lt;<A HREF="mailto:jawbrey@oakland.edu">jawbrey@oakland.edu</A>&gt;</LI>
<LI><em>Sender</em>: <A HREF="mailto:owner-standard-upper-ontology@ieee.org">owner-standard-upper-ontology@ieee.org</A></LI>
</UL>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
<hr>
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<PRE>

¤~~~~~~~~~¤~~~~~~~~~¤~~~~~~~~~¤~~~~~~~~~¤~~~~~~~~~¤

Arisbeans, SUOphists, Countre Kin,

In looking for a fitting bit of background material on Plato's 'Meno',
I found that I had already written what I need here way back at the
outset of my programme in Intelligent Systems Engineering, so here
now is another excerpt from that same old prospectus of mine that
I quoted earlier under the heading of "Quasi-Minds Think Alike",
which was originally titled "Core Meanings &amp; Stand Up Ontology":

<A  HREF="http://ltsc.ieee.org/logs/suo/msg02659.html">http://ltsc.ieee.org/logs/suo/msg02659.html</A>
<A  HREF="http://stderr.org/pipermail/arisbe/2000-December/000094.html">http://stderr.org/pipermail/arisbe/2000-December/000094.html</A>
<A  HREF="http://grouper.ieee.org/groups/ltsc/logs/ontology/msg00253.html">http://grouper.ieee.org/groups/ltsc/logs/ontology/msg00253.html</A>

¤~~~~~~~~~¤~~~~~~~~~¤~SELF~CITATION~¤~~~~~~~~~¤~~~~~~~~~¤

Intelligent Systems Project,
Division 1, Version 4,
September 1, 1992

1.1.2.3  The Trees &amp; The Forest

A sticking point of the whole discussion has just been
reached.  In the idyllic setting of a knowledge field the
question of systematic inquiry takes on the following form:

What piece of code should be followed in order to discover that code?

It is a classic catch, whose pattern was traced out long ago in the paradox
of Plato's 'Meno'.  Discussion of this dialogue and of the task it sets for
AI, cognitive science, education, including the design of intelligent tutoring
systems, can be found in (H. Gardner, 1985), (Chomsky, 1965, '72, '75, '80, '86),
(Fodor, 1975, 1983), (Piattelli-Palmarini, 1980), and in (Collins &amp; Stevens, 1991).
Though it appears to mask a legion of diversions, this text will present itself at
least twice more in the current engagement, both on the horizon and at the gates
of the project to fathom and to build intelligent systems.  Therefore, it is
worth recalling how this inquiry begins.  The interlocutor Meno asks:

| Can you tell me, Socrates, whether virtue can be taught,
| or is acquired by practice, not teaching?  Or if neither
| by practice nor by learning, whether it comes to mankind
| by nature or in some other way?  (Plato, 'Meno', p. 265).

Whether the word "virtue" (arete) is interpreted to mean virtuosity
in a special skill or a more general excellence of conduct, it is
evidently easy, in the understandable rush to "knowledge", to forget
or to ignore what the primary subject of this dialogue is.  Only when
the difficulties of the original question, whether virtue is teachable,
have been moderated by a tentative analysis does knowledge itself become
a topic of the conversation.  This hypothetical mediation of the problem
takes the following tack:  If virtue were a kind of knowledge, and if
every kind of knowledge could be taught, would it not follow that
virtue could be taught?

For the present purpose, it should be recognized that this "trial factorization"
of a problem space or phenomenal field is an important intellectual act in itself,
one that deserves attention in the effort to understand the competencies that
support intelligent functioning.  It is a good question to ask just what sort
of reasoning processes might be involved in the ability to find such a middle
term, as is served by "knowledge" in the example at hand.  Generally speaking,
interest will reside in a whole system of middle terms, which might be called
a "medium" of the problem domain or the field of phenomena.  This usage makes
plain the circumstance that the very recognition and expression of a problem
or phenomenon is already contingent upon and complicit with a particular set
of hypotheses that will inform the direction of its resolution or explanation.

One of the chief theoretical difficulties that obstructs the unification of
logic and dynamics in the study of intelligent systems can be seen in relation
to this question of how an intelligent agent might generate tentative but plausible
analyses of problems that confront it.  As described here, this requires a capacity
for identifying middle grounds that ameliorate or mollify a problem.  This facile
ability does not render any kind of demonstrative argument to be trusted in the
end and for all time, but is a temporizing measure, a way of locating test media
and of trying cases in the media selected.  It is easy to criticize such practices,
to say that every argument should be finally cast into a deductively canonized form,
harder to figure out how to live in the mean time without using such half-measures
of reasoning.  There is a line of thinking, extending from this reference point
in Plato through a glancing remark by Aristotle to the notice of C.S. Peirce,
which holds that the form of reasoning required to accomplish this feat is
neither inductive nor deductive and reduces to no combination of the two,
but is an independent type.
 
Aristotle called this form of reasoning "apagogy" (Prior Analytics, 2.25)
and it was variously translated through the Middle Ages as "reduction" or
"abduction".  The sense of "reduction" here is just that by which one question
or problem is said to reduce to another, as in the AI strategy of goal reduction.
Abductive reasoning is also involved in the initial creation or apt generation of
hypotheses, as in diagnostic reasoning.  Thus, it is natural that abductive reasoning
has periodically become a topic of interest in AI and cognitive modeling, especially
in the effort to build expert systems that simulate and assist diagnosis, whether in
human medicine, auto mechanics, or electronic trouble-shooting.  Recent explorations
in this vein are exemplified by (Peng &amp; Reggia, 1990) and (O'Rorke, 1990).

But there is another reason why the factorization problem presents an especially
acute obstacle to progress in the system-theoretic approach to AI.  When the states
of a system are viewed as a manifold it is usual to imagine that everything factors
nicely into a base manifold and a remainder.  Smooth surfaces come to mind, a single
clear picture of a system that is immanently good for all time.  But this is how an
outside observer might see it, not how it appears to the inquiring system that is
located in a single point and has to discover, starting from there, the most fitting
description of its own space.  The proper division of a state vector into basic and
derivative factors is itself an item of knowledge to be discovered.  It constitutes
a piece of interpretive knowledge that has a large part in determining exactly how
an agent behaves.  The tentative hypotheses that an agent spins out with respect to
this issue will themselves need to be accommodated in a component of free space that
is well under control.  Without a stable theater of action for entertaining hypotheses,
an agent finds it difficult to sustain interest in the kinds of speculative bets that
are required to fund a complex inquiry.

States of information with respect to the placement of this fret or fulcrum can
vary with time.  Indeed, it is a goal of the knowledge directed system to leverage
this chordal node toward optimal possibilities, and this normally requires a continuing
interplay of experimental variations with attunement to the results.  Therefore it seems
necessary to develop a view of manifolds in which the location or depth of the primary
division that is effective in explaining behavior can vary from moment to moment.
The total phenomenal state of a system is its most fundamental reality, but the
way in which these states are connected to make a space, with information that
metes out distances, portrays curvatures, and binds fibers into bundles --
all this is an illusion projected onto the mist of individual states
from items of code in the knowledge component of the current state.
 
The mathematical and computational tools needed to implement such a perspective
goes beyond the understanding of systems and their spaces that I currently have
in my command.  It is considered bad form for a workman to blame his tools, but
in practical terms there continues to be room for better design.  The languages
and media that are made available do, indeed, make some things easier to see,
to say, and to do than others, whether it is English, Pascal (Wirth, 1976),
or Hopi (Whorf, 1956) that is being spoken.  A persistent attention to this
pragmatic factor in epistemology will be necessary to implement the brands
of knowledge-directed systems whose intelligence can function in real time.
To provide a computational language that can help to clarify these problems
is one of the chief theoretical tasks that I see for myself in the work ahead.

A system moving through a knowledge field would ideally be equipped with
a strategy for discovering the structure of that field to the greatest extent
possible.  That ideal strategy is a piece of knowledge, a segment of code existing
in the knowledge space of every point that has this option within its potential.
Does discovery mark only a different awareness of something that already exists,
a changed attitude toward a piece of knowledge already possessed?  Or can it be
something more substantial?  Are genuine invention and proper extensions of the
shared code possible?  Can intelligent systems acquire pieces of knowledge that
are not already in their possession, or in their potential to know?

If a piece of code is near at hand, within a small neighborhood of a system's place in
a knowledge field, then it is easy to see a relationship between adherence and discovery.
It is possible to picture how crumbs of code could be traced back, accumulated, and gradually
reassembled into whole slices of the desired program.  But what if the required code is more
distant?  If a system is observed in fact to drift toward increasing states of knowledge,
does its disposition toward knowledge as a goal need to be explained by some inherent
attraction of knowledge?  Do potential fields and propagating influences have to be
imagined in order to explain the apparent action at a distance?  Do massive bodies
of knowledge then naturally form, and eventually come to dominate whole knowledge
fields?  Are some bodies of knowledge intrinsically more attractive than others?
Can inquiries get so serious that they start to radiate gravity?

Questions like these are only ways of probing the range of possible systems that
are implied by the definition of a knowledge field.  What abstract possibility best
describes a given concrete system is a separate, empirical question.  With luck, the
human situation will be found among the reasonably learnable universes, but before that
hope can be evaluated a lot remains to be discovered about what, in fact, may be learnable
and reasonable.

¤~~~~~~~~~¤~~~~~~~~~¤~NOITATIC~FLES~¤~~~~~~~~~¤~~~~~~~~~¤

Psahmes, The Self-Described Scribe,
Otherwise Known As Utter-Thæn-Wise,

Jon Awbrey

¤~~~~~~~~~¤~~~~~~~~~¤~~~~~~~~~¤~~~~~~~~~¤~~~~~~~~~¤

</PRE>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
<hr>
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="00268" href="msg00268.html">SUO: Re: Arete, Episteme, Techne</a></strong>
<ul><li><em>From:</em> Jon Awbrey &lt;jawbrey@oakland.edu&gt;</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="00256" href="msg00256.html">SUO: Arete, Episteme, Techne</a></strong>
<ul><li><em>From:</em> Jon Awbrey &lt;jawbrey@oakland.edu&gt;</li></ul></li>
<li><strong><a name="00257" href="msg00257.html">SUO: Re: Arete, Episteme, Techne</a></strong>
<ul><li><em>From:</em> Jon Awbrey &lt;jawbrey@oakland.edu&gt;</li></ul></li>
<li><strong><a name="00258" href="msg00258.html">SUO: Re: Arete, Episteme, Techne</a></strong>
<ul><li><em>From:</em> Jon Awbrey &lt;jawbrey@oakland.edu&gt;</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
<ul>
<LI>Prev by Date:
<STRONG><A HREF="msg00261.html">Re[2]: SUO: More KIF-ified Ontology Content</A></STRONG>
</LI>
<LI>Next by Date:
<STRONG><A HREF="msg00259.html">SUO: Ontology Structure &amp; Content</A></STRONG>
</LI>
<li>Prev by thread:
<strong><a href="msg00258.html">SUO: Re: Arete, Episteme, Techne</a></strong>
</li>
<li>Next by thread:
<strong><a href="msg00268.html">SUO: Re: Arete, Episteme, Techne</a></strong>
</li>
<li>Index(es):
<ul>
<li><a href="mail85.html#00260"><strong>Date</strong></a></li>
<li><a href="thrd83.html#00260"><strong>Thread</strong></a></li>
</ul>
</li>
</ul>

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
</body>
</html>
