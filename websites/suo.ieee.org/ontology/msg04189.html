<!-- MHonArc v2.4.8 -->
<!--X-Subject: RE: ONT RE: Ontology case study -->
<!--X-From-R13: Wna @vyrf <vavyrfNgrxabjyrqtr.pbz> -->
<!--X-Date: Fri, 31 May 2002 12:58:51 &#45;0400 (EDT) -->
<!--X-Message-Id: EE25484266A64A47AE06CFC47C64232BA4FF1E@helium.teknowledge.com -->
<!--X-Content-Type: text/plain -->
<!--X-Head-End-->

<!-- /groups/802/3/efm/public/email/msg04189.html -->
<!-- /groups/???? ?SUO?                              -->

<HTML>

<HEAD>
<TITLE>RE: ONT RE: Ontology case study</TITLE>
<LINK REV="made" HREF="mailto:iniles@teknowledge.com">
</HEAD>

<BODY BGCOLOR="#FFFFFF">

<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->

<CENTER>

<TABLE CELLPADDING=3 CELLSPACING=0 BORDER=1 WIDTH="100%">
<TBODY>
<TR ALIGN="CENTER">
<TH COLSPAN=3><STRONG>Thread Links</STRONG></TH>
<TH COLSPAN=3><STRONG>Date Links</STRONG></TH>
</TR>
<TR ALIGN="CENTER">
<TD><A HREF="msg04191.html">Thread Prev</A>
</TD>
<TD><A HREF="msg04192.html">Thread Next</A>
</TD>
<TD><A HREF="thrd7.html#04189">Thread Index</A></Td>
<TD><A HREF="msg04190.html">Date Prev</A></TD>
<TD><A HREF="msg04188.html">Date Next</A>
</TD>
<TD><A HREF="mail7.html#04189">Date Index</A></TD>
</TR>
</TBODY>
</TABLE>
</CENTER>


<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h1>RE: ONT RE: Ontology case study</h1>
<hr>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul>
<li><em>To</em>: &quot;'Chris Partridge'&quot; &lt;<A HREF="mailto:mail@ChrisPartridge.net">mail@ChrisPartridge.net</A>&gt;, <A HREF="mailto:ontology@ieee.org">ontology@ieee.org</A></li>
<li><em>Subject</em>: RE: ONT RE: Ontology case study</li>
<li><em>From</em>: Ian Niles &lt;<A HREF="mailto:iniles@teknowledge.com">iniles@teknowledge.com</A>&gt;</li>
<li><em>Date</em>: Fri, 31 May 2002 09:59:45 -0700</li>
<li><em>Sender</em>: <A HREF="mailto:owner-ontology@majordomo.ieee.org">owner-ontology@majordomo.ieee.org</A></li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
<hr>
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<PRE>

Chris and Adam,

	If you don't mind, I'll chime in here with my thoughts.

-Ian

&gt; -----Original Message-----
&gt; From: Chris Partridge [<A  HREF="mailto:mail@ChrisPartridge.net">mailto:mail@ChrisPartridge.net</A>]
&gt; Sent: Friday, May 31, 2002 12:17 AM
&gt; To: ontology@ieee.org
&gt; Subject: RE: ONT RE: Ontology case study
&gt; 
&gt; 
&gt; 
&gt; Adam,
&gt; 
&gt; AP&gt; Forgive me for being a bit pointed, but I continue to be 
&gt; very troubled
&gt; by
&gt; claims that appear to me unjustified, unspecific or 
&gt; unsupported on this
&gt; list.  If you believe Cyc and SUMO are &quot;not sufficiently 
&gt; accurate&quot;, what
&gt; metric is that in regard to?  Is there a particular axiom in 
&gt; either that
&gt; you could point out that would lead to an incorrect or inaccurate
&gt; conclusion during the course of logical deduction?
&gt; 
&gt; Firstly, as I and a number of other people have pointed out 
&gt; on this list
&gt; many times the issue is not about particular axioms which one 
&gt; can tinker
&gt; with for the rest of our lives - with little effect. 

Hmmm.  This sounds awfully like what one of the Scholastics might have said
to the empirically minded thinkers of the Renaissance - &quot;You can make all of
the observations you like for the rest of your life, but this won't tell us
anything about, for example, how many angels can fit on the end of a pin.&quot;
Really, I don't see that there is any alternative to just sitting down and
building an ontology piece by piece.  As evidence arises that contradicts
some of the axioms, those axioms have to be modified or removed entirely.
As gaps appear in the structure, new axioms have to be written.  It's a slow
and tedious process, but I don't see that there's any alternative.  Many
people wave their hands about &quot;methodology&quot; and a return to &quot;foundations&quot;,
but precious little in the way of concrete principles for ontology
construction have been offered.  To the extent that they have been offered
and they are tenable, they have been adopted.

&gt;My 
&gt; recollection of the
&gt; discussion at the workshop in Seattle was that it was agreed 
&gt; that the top
&gt; levels were not particularly well regimented and that they needed more
&gt; work - one area discussed being, I recall mereology. Your recent post
&gt; admitting no policy for dealing with the difference between 
&gt; attribution and
&gt; exemplification is another example. At the more detailed 
&gt; level, I recall a
&gt; number of people pointed out a number of problems with the notion of
&gt; transaction and process - a point I raised personally with you.
&gt; 
&gt; I am &quot;very troubled&quot; that these issues continue to be ignored.

Actually, these issues have not been ignored.  First, Pierre Grenon of
Cycorp and Tony Cohn of the University of Leeds have reviewed the
mereotopological section of the SUMO in the past few months.  They did
certainly find problems there, but those problems have been addressed.  I'm
sure there are still problems waiting to be discovered, but, as they are
discovered, we will deal with them.  Second, if you want to know where the
SUMO stands with respect to the distinction between &quot;attribution&quot; and
&quot;exemplification&quot;, you need to explain what you mean by these terms and how
they are relevant in this context.  I, for one, don't buy your claim that
database designers modelled their entity/attribute distinction on
Aristotle's substance/attribute dichotomy.  Third, the &quot;problems with the
notion of transaction and process&quot; that were raised at the workshop in
Seattle related to the SUMO assertion that 'Transaction' was a subclass of
both 'Giving' and 'Getting'.  This assertion is no longer in the ontology,
and, in fact, it has been replaced by the following axioms. 

 (=&gt;
   (instance ?TRANS Transaction)
   (exists (?AGENT1 ?AGENT2 ?GIVE1 ?GIVE2 ?OBJ1 ?OBJ2)
      (and
         (instance ?GIVE1 Giving)
         (instance ?GIVE2 Giving)
	   (subProcess ?GIVE1 ?TRANS)
   	   (subProcess ?GIVE2 ?TRANS)
         (agent ?GIVE1 ?AGENT1)
         (agent ?GIVE2 ?AGENT2)
         (patient ?GIVE1 ?OBJ1)
         (patient ?GIVE2 ?OBJ2)
         (destination ?GIVE1 ?AGENT2)
         (destination ?GIVE2 ?AGENT1)
         (not
            (equal ?AGENT1 ?AGENT2))
         (not
            (equal ?OBJ1 ?OBJ2)))))

(=&gt;
   (and
      (instance ?GIVE Giving)
      (agent ?GIVE ?AGENT1)
      (destination ?GIVE ?AGENT2)
      (instance ?AGENT2 Agent)
      (patient ?GIVE ?OBJ))
   (exists (?GET)
      (and
         (instance ?GET Getting)
         (agent ?GET ?AGENT2)
         (origin ?GET ?AGENT1)
         (patient ?GET ?OBJ))))

&gt; 
&gt; To take an example from Cyc, relating to the enterprise, it 
&gt; seems to me from
&gt; the data I have that it regards positions as a role of a 
&gt; person. This is not
&gt; sufficient to track the identity of a position - such as the English
&gt; Monarch. (If I am wrong, I would like to know.) I believe I raised the
&gt; general requirement when the enterprise module of SUMO was 
&gt; being discussed
&gt; and I seem to recall SUMO was amended.

I don't know where Cyc stands on this issue, but in the SUMO 'Position' is a
subclass of 'CognitiveAgent', so that the identity of a position can be
&quot;tracked&quot;.

&gt; 
&gt; CP&gt;It seems to me that if we are looking for the kind of 
&gt; general ontologies
&gt; &gt;that Bill suspects are not feasible, then we need to address 
&gt; the demands
&gt; &gt;of accuracy and regimentation at both the top and domain 
&gt; levels. In my
&gt; &gt;view, for the kind of applications that fall under Bill 
&gt; Anderson s 3), it
&gt; &gt;is only really possible to do these together.
&gt; 
&gt; AP&gt;Indeed.  How would you suggest that we proceed, or proceed 
&gt; differently?
&gt; 
&gt; It seems to me that what the meeting in Seattle agreed - a process of
&gt; regimenting the top level seems sensible. My experience is 
&gt; that the lower
&gt; levels normally benefit from some contact with reality. There are two
&gt; complementary strategies - first extracting (re-engineering) 
&gt; the information
&gt; from existing working systems and secondly using the SUMO to integrate
&gt; systems. Obviously, in both case the goal is not to just find any way
&gt; matching up the systems, but trying to find ways of improving 
&gt; the ontology.

This sounds a lot like the &quot;reflective equilibrium&quot; strategy that we adopted
early on in developing the SUMO.  High level concepts and axioms from
existing ontologies were merged together, and the resulting structure has
been checked by having it reviewed by experts in the field, by developing
domain ontologies that hook into the SUMO, and by mapping the SUMO to all of
the synsets in WordNet.  There is, of course, much more work that needs to
be done in the way of empirically testing the SUMO, but this is generally
true of any endeavor in science or engineering.

&gt; 
&gt; Chris
&gt; 
&gt; 
&gt; 
&gt; -----Original Message-----
&gt; From: Adam Pease [<A  HREF="mailto:apease@ks.teknowledge.com">mailto:apease@ks.teknowledge.com</A>]
&gt; Sent: 30 May 2002 20:41
&gt; To: mail@ChrisPartridge.net; ontology@ieee.org
&gt; Subject: RE: ONT RE: Ontology case study
&gt; 
&gt; Chris,
&gt; 
&gt; At 10:03 PM 5/29/2002 +0200, Chris Partridge wrote:
&gt; 
&gt; &gt;I think the discussion that has developed on the difference 
&gt; between data
&gt; &gt;models and ontologies is linked to Bill Anderson s original 
&gt; comment about
&gt; &gt;the PAR his item 3). It also helps to answer the question Pierluigi
&gt; &gt;Miraglia asked about why certain types of effort are less 
&gt; successful than
&gt; &gt;it seems theoretically they should be. It also helps to answer Bill s
&gt; &gt;point below where I (for once) take Adam s side.
&gt; 
&gt; I'm not sure which side that might be :-)
&gt; 
&gt; 
&gt; &gt;The issue is, it seems to me, accuracy and tolerance.
&gt; &gt;
&gt; &gt;Firstly, the kind of inference in things that the thread is labelling
&gt; &gt;ontologies is absolute without any tolerance. As (real J) 
&gt; engineers know
&gt; &gt;you need to plan for tolerance. An example I like is from (I 
&gt; think) Mike
&gt; &gt;Uschold when building the 777 the errors in the individual 
&gt; tolerances add
&gt; &gt;up leading to a visible difference in the length of the plane and
&gt; &gt;associated problems. A philosopher has made the same point 
&gt; (see pp. 50-1
&gt; &gt;of Dummett s The Logical Basis of Metaphysics 1991), that making
&gt; &gt;inferences tends to dilute precision significantly. I copy 
&gt; the extract
&gt; &gt;below for those that are interested.
&gt; &gt;
&gt; &gt;This means we need techniques to identify which inferences 
&gt; preserve enough
&gt; &gt;accuracy to be workable. The way that this is done in 
&gt; operational systems
&gt; &gt;is two-fold. One the requirements are clear the data is made 
&gt; sufficiently
&gt; &gt;accurate for the specified process (and their inferences). 
&gt; That is why
&gt; &gt;database people are always a bit wary of new processes one 
&gt; of the checks
&gt; &gt;they apply is around data quality. In a system with 
&gt; unrestricted inference
&gt; &gt;the data has to be unrestrictedly accurate i.e. exact.
&gt; &gt;
&gt; &gt;I also note, in passing, that FOL as it stands cannot do 
&gt; what Aristotle
&gt; &gt;called practical reasoning no amount of logical/inferential 
&gt; processing
&gt; &gt;will result in an action. This is not a problem in database systems.
&gt; &gt;
&gt; &gt;This leads onto another problem with what this discussion 
&gt; has labelled
&gt; &gt;ontologies . The strong roots in predicate logic 
&gt; particularly FOL. As is
&gt; &gt;well known (see e.g. p. 48 of Lowe s latest book) predicate logic was
&gt; &gt;developed for mathematic applications and so is not well 
&gt; crafted for more
&gt; &gt;mundane uses. For example, if you believe in a distinction between
&gt; &gt;exemplification and attribution, this is not well marked 
&gt; and, at the very
&gt; &gt;least, the temporality of predication needs some explaining. 
&gt; Note that
&gt; &gt;database systems typically have this distinction built into 
&gt; them but in
&gt; &gt;such a simplistic way that it cannot be practically used to 
&gt; reliably mark
&gt; &gt;the distinction.
&gt; &gt;
&gt; &gt;How does this link to Bill s comment below about the potential for
&gt; &gt;misinterpretation? Firstly, Bill is basically right, without 
&gt; some kind of
&gt; &gt;framework, there will be misinterpretations. However, the work on
&gt; &gt;ontologies may be able to reduce the potential 
&gt; significantly. It seems to
&gt; &gt;me that if one can develop a well founded top ontology that this
&gt; &gt;significantly reduces the potential for some kinds of 
&gt; misinterpretation
&gt; &gt;(NB neither Cyc nor SUMO seem to currently have this).
&gt; 
&gt; Certainly any set of precise definitions that one adds to an 
&gt; existing model
&gt; will help to reduce potential misinterpretation.  One might 
&gt; argue that Cyc
&gt; or SUMO don't do this *sufficiently* with respect to some 
&gt; criteria, or that
&gt; there are some other disadvantages involved in their use, but 
&gt; not that they
&gt; don't add specificity and reduce potential minterpretation.
&gt; 
&gt; &gt;It also seems to me that the domain ontologies also need to be more
&gt; &gt;rigourous. Just as the introduction of databases forced an 
&gt; increased level
&gt; &gt;of accuracy so the sharing of databases (or ontologies) will force a
&gt; &gt;further increase. In the area that I am interested in enterprise
&gt; &gt;ontologies - it seems to me that both Cyc and SUMO are not 
&gt; sufficiently
&gt; &gt;accurate. Of course, Cyc is intended to do common sense reasoning not
&gt; &gt;database integration a different problem, and so has not 
&gt; need for accuracy
&gt; &gt;along this dimension.
&gt; 
&gt; Forgive me for being a bit pointed, but I continue to be very 
&gt; troubled by
&gt; claims that appear to me unjustified, unspecific or 
&gt; unsupported on this
&gt; list.  If you believe Cyc and SUMO are &quot;not sufficiently 
&gt; accurate&quot;, what
&gt; metric is that in regard to?  Is there a particular axiom in 
&gt; either that
&gt; you could point out that would lead to an incorrect or inaccurate
&gt; conclusion during the course of logical deduction?
&gt; 
&gt; &gt;
&gt; &gt;It seems to me that if we are looking for the kind of 
&gt; general ontologies
&gt; &gt;that Bill suspects are not feasible, then we need to address 
&gt; the demands
&gt; &gt;of accuracy and regimentation at both the top and domain 
&gt; levels. In my
&gt; &gt;view, for the kind of applications that fall under Bill 
&gt; Anderson s 3), it
&gt; &gt;is only really possible to do these together.
&gt; 
&gt; Indeed.  How would you suggest that we proceed, or proceed 
&gt; differently?
&gt; 
&gt; Adam
&gt; 
&gt; 
&gt; &gt;Regards,
&gt; &gt;
&gt; &gt;Chris
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;pp. 50-1 - Dummett, M. The Logical Basis of Metaphysics 1991.
&gt; &gt;
&gt; &gt;In a section called Degeneration of Probabilities .
&gt; &gt;
&gt; &gt;Hence it is sufficient, for mathematical purposes, that a 
&gt; principle of
&gt; &gt;inference should guarantee that truth is transmitted from premises to
&gt; &gt;conclusion. Outside mathematics, we have a motive to demand 
&gt; more, if we
&gt; &gt;can get it. ... Most of our beliefs are perforce based upon 
&gt; grounds that
&gt; &gt;fall short of being conclusive, but a form of inference guaranteed to
&gt; &gt;preserve truth is not, in general, guaranteed to preserve degree of
&gt; &gt;probability. ... The 'ideal' subject starting from beliefs whose
&gt; &gt;probability is close to 1, will end up with beliefs 
&gt; negligibly greater
&gt; &gt;than 0; the man of common sense, initially adopting beliefs 
&gt; with a much
&gt; &gt;weaker evidential basis, but reasoning from them only to a 
&gt; meagre extent,
&gt; &gt;will finish with far fewer beliefs than he. That is why scientific
&gt; &gt;conclusions arrived at by long chains of impeccable reasoning almost
&gt; &gt;always prove, when a direct test becomes possible, to be 
&gt; wrong. ... In
&gt; &gt;practical life, truth is valued chiefly as a guide to 
&gt; action; and then the
&gt; &gt;principal remedy for the degeneration of probability in the course of
&gt; &gt;inferential reasoning is to employ it sparingly.
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;-----Original Message-----
&gt; &gt;From: owner-ontology@majordomo.ieee.org
&gt; &gt;[<A  HREF="mailto:owner-ontology@majordomo.ieee.org]On">mailto:owner-ontology@majordomo.ieee.org]On</A> Behalf Of 
&gt; William Burkett
&gt; &gt;Sent: 29 May 2002 17:15
&gt; &gt;To: 'Adam Pease'
&gt; &gt;Cc: ontology@ieee.org
&gt; &gt;Subject: ONT RE: Ontology case study
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;Hi, Adam --
&gt; &gt;
&gt; &gt; &gt; &gt; -----Original Message-----
&gt; &gt; &gt; &gt; &gt; From: Adam Pease [<A  HREF="mailto:apease@ks.teknowledge.com">mailto:apease@ks.teknowledge.com</A>]
&gt; &gt; &gt; &gt; &gt; Sent: Thursday, May 23, 2002 2:10 PM
&gt; &gt; &gt; &gt; &gt; To: William Burkett; ontology@ieee.org
&gt; &gt; &gt; &gt; &gt; Subject: RE: Ontology case study
&gt; &gt; &gt; &gt;...
&gt; &gt; &gt; &gt; &gt; &gt;*The* most significant problem with this paradigm, 
&gt; however, is the
&gt; &gt; &gt; &gt; &gt; &gt;development and application of mappings.  What is &quot;mapping&quot;,
&gt; &gt; really?  Can
&gt; &gt; &gt; &gt; &gt; &gt;it be understood and taught to the general ontology-using
&gt; &gt; public?  Your
&gt; &gt; &gt; &gt; &gt; &gt;effort was successful because you were dealing with a closed
&gt; &gt; system of a
&gt; &gt; &gt; &gt; &gt; &gt;known and well-defined scope and data meanings.  How can the
&gt; mapping
&gt; &gt; &gt; &gt; &gt; &gt;lessons you learned (and were learned in the above 
&gt; efforts) be
&gt; &gt; applied to
&gt; &gt; &gt; &gt; &gt; &gt;an open system with a huge, unknown, and constantly 
&gt; evolving scope
&gt; &gt; and
&gt; &gt; &gt; &gt; &gt; &gt;fuzzy, ambiguous, context-sensitive data meanings?
&gt; &gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; &gt; The mapping problem is significant, to be sure, but 
&gt; is a problem in
&gt; &gt; any
&gt; &gt; &gt; &gt; &gt; sort of integration effort, whether using ontologies, 
&gt; or a more
&gt; &gt; &gt; &gt; &gt; conventional data warehouse approach.  I would suggest that at
&gt; &gt; least the
&gt; &gt; &gt; &gt; &gt; problem is more manageable than typical systems integration
&gt; approaches
&gt; &gt; &gt; &gt; &gt; where n components require n^2 mappings.
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt;While I agree that mapping is an issue regardless of the 
&gt; integration
&gt; &gt; &gt; &gt;paradigm/approach used (e.g., neutral model, data 
&gt; warehouse, database
&gt; &gt; &gt; &gt;federations), I don't agree that the neutral model offers any
&gt; advantages
&gt; &gt; &gt; &gt;in terms of manageability.  In fact, I think the problem 
&gt; is actually
&gt; far
&gt; &gt; &gt; &gt;more complex and less manageable than n^2 direct 
&gt; mappings.  Sure, you
&gt; &gt; &gt; &gt;reduce the number of mappings to 2 * n, but then you have to
&gt; &gt; &gt; deal with:
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt;   - loss of semantic precision when &quot;generalizing&quot; 
&gt; local data into the
&gt; &gt; &gt; &gt; neutral model, making extraction (interpretation) of 
&gt; meaning in the
&gt; &gt; &gt; &gt; neutral model by other connected data source imprecise or wrong.
&gt; &gt; &gt;
&gt; &gt; &gt; That could be a problem if the neutral model is not 
&gt; specific or detailed
&gt; &gt; &gt; enough.  That needn't be the case however.
&gt; &gt;
&gt; &gt;I feel I should respond to this indirectly, because - on reading your
&gt; &gt;other responses - I feel that the following observation is 
&gt; the source of
&gt; &gt;many of our differences of opinion.
&gt; &gt;
&gt; &gt;I think that our differences stem from differing assumptions 
&gt; about how an
&gt; &gt;upper ontology is to be or will be used (a neutral 
&gt; integration model being
&gt; &gt;one such use.)  Your statement here implies that it is 
&gt; *possible* to be
&gt; &gt;detailed and specific enough that the upper ontology/neutral 
&gt; model either
&gt; &gt;cannot or will not be misused.  (Matthew, in his response, 
&gt; makes the same
&gt; &gt;assumption.)  I feel this assumption is -- forgive me for 
&gt; saying and I
&gt; &gt;intend no offense -- both naive and dogmatic.  If &quot;people&quot; 
&gt; use the upper
&gt; &gt;ontology/neutral model at all, they WILL misuse it and 
&gt; interpret it to
&gt; &gt;their own needs - knowingly or unknowingly.  All of my data modelling
&gt; &gt;experience leads me to this position.
&gt; &gt;
&gt; &gt;I envision a bunch of different communities of people 
&gt; creating a bunch of
&gt; &gt;ontologies and mapping them together following some 
&gt; standardized protocols
&gt; &gt;such that a &quot;knowledge web&quot; can be built up incrementally as 
&gt; people do
&gt; &gt;their jobs locally (like the internet has grown based on a 
&gt; few standard
&gt; &gt;protocols).  I suspect that your vision is not dissimilar, 
&gt; though I cannot
&gt; &gt;imagine what safeguards or procedures could possibly be put 
&gt; in place to
&gt; &gt;prevent misuse of an upper ontology without some single 
&gt; overseeing arbiter
&gt; &gt;to police its use.  (I also don't know if this &quot;knowledge 
&gt; web&quot; is the same
&gt; &gt;thing as the &quot;Semantic Web&quot;, though it's as valid an 
&gt; interpretation or
&gt; &gt;vision as any.)
&gt; &gt;
&gt; &gt;Perhaps another important differing assumption is that you/Matthew
&gt; &gt;assumption computing or modelling professionals will be 
&gt; interpreting the
&gt; &gt;upper ontology/neutral model and, therefore, have the 
&gt; responsibility of
&gt; &gt;using it correctly.  I can't argue with this.  My 
&gt; assumption, however, is
&gt; &gt;that Joe Everyman can pick it up, use it, or create his own 
&gt; ontology if he
&gt; &gt;wants to get what he knows into a computer.
&gt; &gt;
&gt; &gt;We want the cost-of-entry for using an upper ontology to be low,
&gt; &gt;right?  We can't, therefore, assume or depend on people using it
&gt; &gt;correctly; rather, we should build in the safe-fail features 
&gt; to make sure
&gt; &gt;that when it fails, such a fail isn't disastrous and 
&gt; recovery operations
&gt; &gt;can be immediately started.
&gt; &gt;
&gt; &gt; &gt;
&gt; &gt; &gt; &gt;   - Mapping &quot;data source A&quot; -&gt; &quot;neutral model NM&quot; and 
&gt; &quot;data source
&gt; B&quot; -&gt;
&gt; &gt; &gt; &gt; &quot;neutral model NM&quot; is not the same mapping &quot;data source 
&gt; A + B&quot; -&gt;
&gt; &gt; &gt; &gt; &quot;neutral model NM&quot;.  If there's an overlap of 
&gt; information in A and B,
&gt; &gt; &gt; &gt; there's a kind of &quot;information multiplexing&quot; involved that makes
&gt; correct
&gt; &gt; &gt; &gt; mapping more difficult.
&gt; &gt; &gt;
&gt; &gt; &gt; I'm not sure I understand.  Could you provide an example?
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;Suppose I map data from a local driver's registration database (data
&gt; &gt;source A) into state government database for motor-voter registration
&gt; &gt;(data source NM).  Suppose then that data from the 
&gt; Department of Motor
&gt; &gt;Vehicles (data source B) is also mapped into the state database.  If
&gt; &gt;William Burkett in Los Angeles is translated from A into NM, 
&gt; there will be
&gt; &gt;a motor-voter record for that individual created.  If 
&gt; William C. Burkett
&gt; &gt;from Los Angeles County is mapped from data source C into 
&gt; NM, then is a
&gt; &gt;new motor-voter individual record created?  If I'd had prior 
&gt; knowledge of
&gt; &gt;the two data sources - A and B - I could write mapping rules 
&gt; that take
&gt; &gt;into account the fact that data source A doesn't use middle 
&gt; initials and
&gt; &gt;that if (1) first name-last name and (2) city is in county, 
&gt; the it's one
&gt; &gt;and the same individual.  If A and B are both mapped 
&gt; independantly of the
&gt; &gt;knowledge of the other, it's very likely that there will be 
&gt; two records
&gt; &gt;for the same individual in the NM.
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt; &gt;
&gt; &gt; &gt; &gt;   - Mapping between A and B is straight-forward because 
&gt; the &quot;system&quot;
&gt; is
&gt; &gt; &gt; &gt; essentially closed: you know what is in A and what is 
&gt; in B.  Mapping
&gt; &gt; A to
&gt; &gt; &gt; &gt; a NM is less deterministic: you *think* you know what 
&gt; is in NM, but if
&gt; &gt; &gt; &gt; others are free to map to it, their interpretation of 
&gt; what is in the
&gt; NM
&gt; &gt; &gt; &gt; will likely be very different from yours.  In other words, the
&gt; &gt; assumption
&gt; &gt; &gt; &gt; that all mappers will interpret the NM in the same way 
&gt; while mapping
&gt; is
&gt; &gt; &gt; &gt; false. (Hell - the assumption that any two people will 
&gt; interpret *any*
&gt; &gt; &gt; &gt; model the same way is probably false, too.)
&gt; &gt; &gt;
&gt; &gt; &gt; I believe this is actually a good counterexample.  While 
&gt; the terms and
&gt; &gt; &gt; relations in a database representation don't have a 
&gt; formal semantics
&gt; (note
&gt; &gt; &gt; that I didn't say SQL itself doesn't have a formal semantics),
&gt; axiomatized
&gt; &gt; &gt; terms and relations in first order logic do.  The axioms  
&gt; completely
&gt; &gt; specify
&gt; &gt; &gt; the meaning of the term so there is not as much of an issue about
&gt; people's
&gt; &gt; &gt; different interpretations.  Of course, if the axioms are 
&gt; not detailed or
&gt; &gt; &gt; specific enough that's a problem just as it would be with any
&gt; &gt; &gt; underspecified representation.
&gt; &gt;
&gt; &gt;I think THE most important issue is people's different
&gt; &gt;interpretations.  (See my second assumption above.)  
&gt; Regardless of the FOL
&gt; &gt;language chosen to represent the ontology, human beings are 
&gt; still going to
&gt; &gt;read the words/terms that are tokens in the FOL 
&gt; representation and apply
&gt; &gt;natural language interpretations to them.  There is no 
&gt; getting away from
&gt; &gt;this -- we are trapped using natural language - ultimately - 
&gt; to articulate
&gt; &gt;and interpret meaning (i.e., real world domain semantics).
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt; &gt; &gt;   - When a new &quot;node&quot; is added to the community of 
&gt; integrated &quot;nodes&quot;
&gt; &gt; &gt; &gt; mapped to a common NM, the mappings of all the nodes need to be
&gt; reviewed
&gt; &gt; &gt; &gt; to see if they still &quot;interpret&quot; the NM properly given 
&gt; the expansion
&gt; of
&gt; &gt; &gt; &gt; its semantic applicability with/for the new node.
&gt; &gt; &gt;
&gt; &gt; &gt; I would have to disagree with this as well.  The 
&gt; interpretation of a
&gt; term
&gt; &gt; &gt; does not change just because some additional term is added to the
&gt; &gt; &gt; ontology.  All the past mappings would still be correct.  
&gt; The only issue
&gt; &gt; &gt; would be whether the mappings are specific enough and 
&gt; take appropriate
&gt; &gt; &gt; advantage of the presence of a new term.
&gt; &gt;
&gt; &gt;I guess we'll have to settle for the old &quot;agree to disagree&quot; 
&gt; conclusion,
&gt; &gt;then, because my fundamental assumptions lead me to the opposite
&gt; &gt;position.  I think it is very likely, in not inevitable, that the
&gt; &gt;interpretation of a new term could cause &quot;interpretive 
&gt; ripples&quot; through a
&gt; &gt;collection of mapped ontologies.  It's the same phenomena as 
&gt; a new person
&gt; &gt;coming into your committee meeting half-way through: there's 
&gt; a temporary
&gt; &gt;pertubation of the discussion while the new person &quot;comes up 
&gt; to speed&quot;
&gt; &gt;with what's transpired so that he/she can then fully and fruitfully
&gt; &gt;participate and contribute.
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt; &gt;
&gt; &gt; &gt; &gt;Off the top of my head, these are just some of the 
&gt; problems with a
&gt; &gt; neutral
&gt; &gt; &gt; &gt;model integration approach.  These problems can be overcome
&gt; &gt; &gt; &gt;methodologically, of course, but the depth and dimensions of the
&gt; problem
&gt; &gt; &gt; &gt;are, I think, still poorly understood (if not mostly 
&gt; unrecognized).
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; &gt; &gt;While I think you can sell the neutral ontology 
&gt; integration model
&gt; &gt; as a
&gt; &gt; &gt; &gt; &gt; &gt;problem solving approach, getting people to know 
&gt; about and use
&gt; &gt; SUMO (or
&gt; &gt; &gt; &gt; &gt; &gt;any other &quot;upper&quot; ontology) as neutral ontology in 
&gt; their solution
&gt; &gt; is a
&gt; &gt; &gt; &gt; &gt; &gt;different kind of sales job altogether.  And it is one that I
&gt; &gt; don't think
&gt; &gt; &gt; &gt; &gt; &gt;will be very successful - any well-defined and well-bounded
&gt; &gt; integration
&gt; &gt; &gt; &gt; &gt; &gt;effort will want to use their own.
&gt; &gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt; &gt; Can you discuss further why you feel they'd want to use their
&gt; &gt; own?  I've
&gt; &gt; &gt; &gt; &gt; found that unlike in the research world, people who want to
&gt; &gt; accomplish a
&gt; &gt; &gt; &gt; &gt; practical commercial task are very happy to adopt 
&gt; someone else's
&gt; &gt; models or
&gt; &gt; &gt; &gt; &gt; software if it helps them get their job done.
&gt; &gt; &gt; &gt;
&gt; &gt; &gt; &gt;But in adopting someone else's models or software, how 
&gt; often do they
&gt; use
&gt; &gt; &gt; &gt;them exactly as is?  I don't know how often I've heard &quot;My/our
&gt; &gt; &gt; &gt;requirements are different&quot;.  At the very best, they 
&gt; would use the
&gt; &gt; &gt; &gt;models/software as a starting point for doing what they want to
&gt; &gt; &gt; &gt;do.  Adopting and adapting a neutral ontology model to 
&gt; the usage and
&gt; &gt; needs
&gt; &gt; &gt; &gt;of your local (integrated) community defeats the whole purpose of
&gt; &gt; using it
&gt; &gt; &gt; &gt;as a generalized integration model.  People will 
&gt; interpret and use the
&gt; &gt; &gt; &gt;model as they wish, and this can't be policed (and 
&gt; shouldn't because
&gt; &gt; it is
&gt; &gt; &gt; &gt;not wrong of them do this - it's natural.)  The only way 
&gt; &quot;standardized&quot;
&gt; &gt; &gt; &gt;interpretations will arise is by the conventions that 
&gt; arise and are
&gt; &gt; &gt; &gt;reinforced in a language-use community, in which case it 
&gt; will pay for
&gt; &gt; &gt; &gt;people to interpret the ontology the same way.  
&gt; (Remember: dictionaries
&gt; &gt; &gt; &gt;don't specify the meaning of words; they document the 
&gt; conventional
&gt; &gt; &gt; &gt;meanings of usages of a word.)
&gt; &gt; &gt;
&gt; &gt; &gt; Well, we're drawing on the anecdotes of personal 
&gt; experience here, not
&gt; &gt; &gt; having the results of some survey that specifies how 
&gt; various groups of
&gt; &gt; &gt; people use various types of software.
&gt; &gt;
&gt; &gt;True enough.  I know of little realistic &quot;research studies&quot; 
&gt; in this field.
&gt; &gt;
&gt; &gt; &gt;I would only try to support my view
&gt; &gt; &gt; further with the fact that the vast majority of Java 
&gt; programmers use,
&gt; and
&gt; &gt; &gt; subclass the JDK, rather than feeling a need to modify it.
&gt; &gt;
&gt; &gt;And my view stems from a different set of experiences, e.g., 
&gt; modelling the
&gt; &gt;information requirements of domain experts for the purpose 
&gt; of representing
&gt; &gt;and exchanging data between CAD systems.  My response to 
&gt; your Java example
&gt; &gt;is that Java and the behavior of computing machines is a (relatively)
&gt; &gt;well-understood domain compared to the knowledge in the &quot;real
&gt; &gt;world&quot;.  Therefore the programmers that subclass the JDK 
&gt; already know what
&gt; &gt;the classes could/should do and how to apply them.  You give the same
&gt; &gt;programmers a class diagram ostensbility representing domain 
&gt; knowledge,
&gt; &gt;like people or parts or products, and they will each interpret them -
&gt; &gt;differently - as they see fit in their applications.  (Again 
&gt; - that has
&gt; &gt;been my consistent and unvarying experience.)
&gt; &gt;
&gt; &gt;
&gt; &gt;
&gt; &gt;--- Bill
&gt; 
&gt; Adam Pease
&gt; Teknowledge
&gt; (650) 424-0500 x571
&gt; 
&gt; 

</PRE>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
<hr>
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="04192" href="msg04192.html">RE: ONT RE: Ontology case study</a></strong>
<ul><li><em>From:</em> &quot;Chris Partridge&quot; &lt;mail@ChrisPartridge.net&gt;</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<!--X-References-End-->
<!--X-BotPNI-->
<ul>
<LI>Prev by Date:
<STRONG><A HREF="msg04190.html">RE: ONT RE: Ontology case study</A></STRONG>
</LI>
<LI>Next by Date:
<STRONG><A HREF="msg04188.html">Re: ONT RE: Ontology case study</A></STRONG>
</LI>
<li>Prev by thread:
<strong><a href="msg04191.html">Re: ONT RE: Ontology case study</a></strong>
</li>
<li>Next by thread:
<strong><a href="msg04192.html">RE: ONT RE: Ontology case study</a></strong>
</li>
<li>Index(es):
<ul>
<li><a href="mail7.html#04189"><strong>Date</strong></a></li>
<li><a href="thrd7.html#04189"><strong>Thread</strong></a></li>
</ul>
</li>
</ul>

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
</body>
</html>
